
# ğŸ§ Abstractive Summarization Using GRU & LSTM

This project implements and compares GRU and LSTM-based sequence-to-sequence models with attention mechanisms for the task of **abstractive text summarization**, using the WikiHow dataset.

---

## ğŸ” Project Highlights

- ğŸ“˜ **Models:** GRU and LSTM encoder-decoder architectures with attention
- ğŸ“Š **Evaluation:** ROUGE-1, ROUGE-2 and ROUGE-L scores
- ğŸ§ª **Dataset:** WikiHow Abstractive Summarization Dataset (cleaned sample used)
- ğŸ“ˆ **Results:** LSTM with attention slightly outperforms GRU in ROUGE metrics
- ğŸ“ **Outputs:** Summary predictions stored in CSV format for both models

---

## ğŸ—‚ï¸ Repository Structure

```
ğŸ“ notebooks/          - GRU & LSTM model development and outputs
ğŸ“ src/                - Modular Python scripts for model, preprocessing, attention
ğŸ“ outputs/            - ROUGE scores and sample predictions
ğŸ“ data/               - Sample WikiHow input/output examples
ğŸ“„ requirements.txt    - Python dependencies
ğŸ“„ README.md           - Project overview and setup instructions
```

## ğŸ“ˆ Example Results

| Model             | ROUGE-1 | ROUGE-2 | ROUGE-L |
|------------------|---------|---------|---------|
| GRU + Attention   | 0.3345  | 0.1527  | 0.3079  |
| LSTM + Attention  | 0.3421  | 0.1598  | 0.3185  |

---

## ğŸ“¦ Sample Output

**Input:** how to make tea  
**GRU Summary:** Boil water, steep bag, serve tea.  
**LSTM Summary:** Boil water, steep tea, add sugar.

---

## ğŸ¤ Authors

- [Anuraag Reddy Kommareddy](mailto:akomm@uic.edu)  
- [Nirmal Kumar Varma Vegesna](mailto:nveges2@uic.edu)
